{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-20T08:26:12.361069500Z",
     "start_time": "2025-03-20T08:26:12.338403400Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Area', 'Item', 'Year', 'hg/ha_yield',\n",
      "       'average_rain_fall_mm_per_year', 'pesticides_tonnes', 'avg_temp'],\n",
      "      dtype='object')\n",
      "['Maize' 'Potatoes' 'Rice, paddy' 'Sorghum' 'Soybeans' 'Wheat' 'Cassava'\n",
      " 'Sweet potatoes' 'Plantains and others' 'Yams']\n",
      "Potatoes                4276\n",
      "Maize                   4121\n",
      "Wheat                   3857\n",
      "Rice, paddy             3388\n",
      "Soybeans                3223\n",
      "Sorghum                 3039\n",
      "Sweet potatoes          2890\n",
      "Cassava                 2045\n",
      "Yams                     847\n",
      "Plantains and others     556\n",
      "Name: Item, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# read data from csv\n",
    "filename = '../../dataset/crops_dataset/yield_df.csv'\n",
    "yield_dataframe = pd.read_csv(filename)\n",
    "#check data \n",
    "#print(yield_dataframe.head())\n",
    "# check coulmns\n",
    "print(yield_dataframe.columns)\n",
    "# checking unique values for each column\n",
    "print(yield_dataframe['Item'].unique())\n",
    "print(yield_dataframe['Item'].value_counts())\n",
    "# making copy of dataframe\n",
    "dataframe = yield_dataframe.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-20T09:16:57.147655800Z",
     "start_time": "2025-03-20T09:16:57.026880200Z"
    }
   },
   "id": "3de7199871db1563",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Item', 'Year', 'hg/ha_yield', 'average_rain_fall_mm_per_year',\n",
      "       'pesticides_tonnes', 'avg_temp'],\n",
      "      dtype='object')\n",
      "Item                              object\n",
      "Year                               int64\n",
      "hg/ha_yield                        int64\n",
      "average_rain_fall_mm_per_year    float64\n",
      "pesticides_tonnes                float64\n",
      "avg_temp                         float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "## we are going to make a general model so the countries from which the plantation \n",
    "## was made doesn't matter\n",
    "## dataset cleaning\n",
    "#dataframe.drop(columns=\"Unnamed: 0\",inplace=True)\n",
    "print(dataframe.columns)\n",
    "##deleting duplicates\n",
    "# dataframe.drop_duplicates(inplace=True)\n",
    "## checking null values\n",
    "#print(dataframe.isnull().sum())\n",
    "## checking datatypes\n",
    "print(dataframe.dtypes)\n",
    "# transforming datatypes for some field like ( Items )\n",
    "item_frequency = dataframe['Item'].value_counts()\n",
    "dataframe['crop_frequency'] = dataframe['Item'].map(item_frequency)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-20T09:20:37.033492400Z",
     "start_time": "2025-03-20T09:20:37.018445600Z"
    }
   },
   "id": "18f3d3de5511e0cc",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potatoes                4276\n",
      "Maize                   4121\n",
      "Wheat                   3857\n",
      "Rice, paddy             3388\n",
      "Soybeans                3223\n",
      "Sorghum                 3039\n",
      "Sweet potatoes          2890\n",
      "Cassava                 2045\n",
      "Yams                     847\n",
      "Plantains and others     556\n",
      "Name: Item, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# dataframe.drop(columns=[\"Year\"],inplace=True)\n",
    "print(dataframe[\"Item\"].value_counts())\n",
    "\n",
    "# print(dataframe.head())\n",
    "#dataframe.corr(numeric_only=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-20T18:28:36.941531900Z",
     "start_time": "2025-03-20T18:28:36.928863400Z"
    }
   },
   "id": "179c92fcf55259de",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "16fa59ef2221e473"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09162971834426967\n"
     ]
    }
   ],
   "source": [
    "## scaling data\n",
    "'''\n",
    "    we are going to use standardization for feature scaling \n",
    "    but because scaling the model on the whole dataset can cause the model to over fit and data leakage\n",
    "    so let us first split data into training and testing data\n",
    "'''\n",
    "## splitting data\n",
    "X = dataframe[[\"crop_frequency\",\"pesticides_tonnes\",\"avg_temp\",\"average_rain_fall_mm_per_year\"]]\n",
    "Y = dataframe[\"hg/ha_yield\"] \n",
    "\n",
    "'''\n",
    "    the thing is that we are saying we will train our model that give x_train value the results will be y_train\n",
    "    then to test the model we will ask the model that what would be the output when given this x_test values \n",
    "    and then later the it will evaluates its self of the y_test values\n",
    "'''\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=43)\n",
    "\n",
    "'''\n",
    "Fit:\n",
    "\n",
    "The scaler computes the mean and standard deviation of the training data.\n",
    "Transform:\n",
    "The scaler applies the transformation to the data using the formula\n",
    "The transform method applies a precomputed transformation to new data. It does not recompute statistics (e.g., mean,standard deviation); instead, it uses the statistics already computed during the fit step\n",
    "\n",
    "'''\n",
    "# scaling\n",
    "scaler =  StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# model training\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=43)\n",
    "model.fit(x_train_scaled,y_train)\n",
    "\n",
    "# predicting \n",
    "y_pred = model.predict(x_test)\n",
    "print(y_pred)\n",
    "\n",
    "# model evaluation\n",
    "MAE = mean_squared_error()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-18T09:07:13.324442100Z",
     "start_time": "2025-03-18T09:07:13.266241400Z"
    }
   },
   "id": "8df5401b7f4b5e4c",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f5bf943acb43d44e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
